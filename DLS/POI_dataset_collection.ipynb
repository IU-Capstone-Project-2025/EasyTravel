{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-17T09:34:12.422066Z",
     "start_time": "2025-06-17T09:34:09.674748Z"
    }
   },
   "source": [
    "!pip install requests pandas\n",
    "!pip install tqdm"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\emil1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\emil1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\emil1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\emil1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\emil1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\emil1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (2025.4.26)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\emil1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\emil1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\emil1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\emil1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\emil1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\emil1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\emil1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tqdm) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T11:14:04.062764Z",
     "start_time": "2025-06-17T09:34:12.434651Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "OVERPASS_URL = \"http://overpass-api.de/api/interpreter\"\n",
    "WIKIPEDIA_API_URL = \"https://ru.wikipedia.org/w/api.php\"\n",
    "WIKIDATA_API_URL = \"https://www.wikidata.org/w/api.php\"\n",
    "\n",
    "all_processed_pois = []\n",
    "\n",
    "def get_wikipedia_summary(title):\n",
    "    \"\"\"–ü–æ–ª—É—á–∞–µ—Ç –∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –∏–∑ –í–∏–∫–∏–ø–µ–¥–∏–∏ –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é —Å—Ç–∞—Ç—å–∏.\"\"\"\n",
    "    try:\n",
    "        params = {\n",
    "            \"action\": \"query\",\n",
    "            \"format\": \"json\",\n",
    "            \"titles\": title,\n",
    "            \"prop\": \"extracts\",\n",
    "            \"exintro\": True,\n",
    "            \"explaintext\": True,\n",
    "            \"redirects\": 1\n",
    "        }\n",
    "        response = requests.get(WIKIPEDIA_API_URL, params=params, timeout=10)\n",
    "        data = response.json()\n",
    "        page_id = next(iter(data['query']['pages']))\n",
    "        if page_id != '-1':\n",
    "            return data['query']['pages'][page_id].get('extract', '')\n",
    "        return \"\"\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(f\"–¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ –í–∏–∫–∏–ø–µ–¥–∏–∏ –¥–ª—è {title}\")\n",
    "        return \"\"\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        return \"\"\n",
    "\n",
    "def get_wikidata_description(qid):\n",
    "    try:\n",
    "        params = {\n",
    "            \"action\": \"wbgetentities\",\n",
    "            \"format\": \"json\",\n",
    "            \"ids\": qid,\n",
    "            \"props\": \"descriptions\",\n",
    "            \"languages\": \"ru|en\"\n",
    "        }\n",
    "        response = requests.get(WIKIDATA_API_URL, params=params, timeout=10)\n",
    "        data = response.json()\n",
    "        entity = data['entities'].get(qid)\n",
    "        if entity and 'descriptions' in entity:\n",
    "            if 'ru' in entity['descriptions']:\n",
    "                return entity['descriptions']['ru']['value']\n",
    "            elif 'en' in entity['descriptions']:\n",
    "                return entity['descriptions']['en']['value']\n",
    "        return \"\"\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(f\"–¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ Wikidata –¥–ª—è {qid}\")\n",
    "        return \"\"\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        return \"\"\n",
    "\n",
    "def create_text_description(tags):\n",
    "    description_parts = []\n",
    "\n",
    "    if 'name' in tags:\n",
    "        description_parts.append(tags['name'])\n",
    "\n",
    "    tag_translations = {\n",
    "        'amenity': '–¢–∏–ø',\n",
    "        'shop': '–ú–∞–≥–∞–∑–∏–Ω',\n",
    "        'tourism': '–¢—É—Ä–∏–∑–º',\n",
    "        'leisure': '–î–æ—Å—É–≥',\n",
    "        'historic': '–ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–π –æ–±—ä–µ–∫—Ç',\n",
    "        'cuisine': '–ö—É—Ö–Ω—è'\n",
    "    }\n",
    "\n",
    "    for key, name in tag_translations.items():\n",
    "        if key in tags:\n",
    "            description_parts.append(f\"{name}: {tags[key].replace('_', ' ')}\")\n",
    "\n",
    "    if 'opening_hours' in tags:\n",
    "        description_parts.append(f\"–ß–∞—Å—ã —Ä–∞–±–æ—Ç—ã: {tags['opening_hours']}\")\n",
    "    if 'wheelchair' in tags and tags['wheelchair'] == 'yes':\n",
    "        description_parts.append(\"–î–æ—Å—Ç—É–ø–Ω–æ –¥–ª—è –∏–Ω–≤–∞–ª–∏–¥–Ω—ã—Ö –∫–æ–ª—è—Å–æ–∫\")\n",
    "    if 'internet_access' in tags and tags['internet_access'] == 'yes':\n",
    "        description_parts.append(\"–ï—Å—Ç—å –¥–æ—Å—Ç—É–ø –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç\")\n",
    "    if 'phone' in tags:\n",
    "        description_parts.append(f\"–¢–µ–ª–µ—Ñ–æ–Ω: {tags['phone']}\")\n",
    "    if 'website' in tags:\n",
    "        description_parts.append(f\"–í–µ–±—Å–∞–π—Ç: {tags['website']}\")\n",
    "\n",
    "    if 'description' in tags:\n",
    "        description_parts.append(f\"–û–ø–∏—Å–∞–Ω–∏–µ: {tags['description']}\")\n",
    "\n",
    "    extra_description = \"\"\n",
    "    if 'wikipedia' in tags:\n",
    "        parts = tags['wikipedia'].split(':')\n",
    "        if len(parts) >= 2:\n",
    "            lang = parts[0]\n",
    "            title = parts[-1]\n",
    "            if lang == 'ru':\n",
    "                wiki_summary = get_wikipedia_summary(title)\n",
    "                if wiki_summary:\n",
    "                    extra_description = wiki_summary\n",
    "    elif 'wikidata' in tags:\n",
    "        wikidata_id = tags['wikidata']\n",
    "        wiki_description = get_wikidata_description(wikidata_id)\n",
    "        if wiki_description:\n",
    "            extra_description = wiki_description\n",
    "\n",
    "    if extra_description:\n",
    "        description_parts.append(extra_description)\n",
    "\n",
    "    return \". \".join(description_parts)\n",
    "\n",
    "def process_city_pois(city_name):\n",
    "    print(f\"\\n--- –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –≥–æ—Ä–æ–¥–∞: {city_name} ---\")\n",
    "    overpass_query = f\"\"\"\n",
    "    [out:json][timeout:180];\n",
    "    area[name=\"{city_name}\"]->.searchArea;\n",
    "    (\n",
    "      node[\"tourism\"](area.searchArea);\n",
    "      way[\"tourism\"](area.searchArea);\n",
    "\n",
    "      node[\"leisure\"](area.searchArea);\n",
    "      way[\"leisure\"](area.searchArea);\n",
    "\n",
    "      node[\"historic\"](area.searchArea);\n",
    "      way[\"historic\"](area.searchArea);\n",
    "\n",
    "      node[\"place\"~\"square|fountain\"](area.searchArea);\n",
    "      way[\"place\"~\"square|fountain\"](area.searchArea);\n",
    "\n",
    "      node[\"natural\"~\"park|wood|garden|beach|peak|water\"](area.searchArea);\n",
    "      way[\"natural\"~\"park|wood|garden|beach|peak|water\"](area.searchArea);\n",
    "\n",
    "      node[\"amenity\"~\"arts_centre|theatre|cinema|museum|library|nightclub|bar|restaurant|cafe|pub|food_court|community_centre|marketplace|atm|bank|clinic|hospital|pharmacy|post_office|police|fire_station|school|university|kindergarten|dentist|veterinary|parking|toilets|fountain|place_of_worship|courthouse|embassy|townhall|public_bath|sauna|stripclub|brothel\"](area.searchArea);\n",
    "      way[\"amenity\"~\"arts_centre|theatre|cinema|museum|library|nightclub|bar|restaurant|cafe|pub|food_court|community_centre|marketplace|place_of_worship\"](area.searchArea);\n",
    "      node[\"amenity\"~\"^(arts_centre|theatre|cinema|museum|library|nightclub|bar|restaurant|cafe|pub|food_court|marketplace|fountain|public_bath|sauna|stripclub|brothel)$\"](area.searchArea);\n",
    "      way[\"amenity\"~\"^(arts_centre|theatre|cinema|museum|library|nightclub|bar|restaurant|cafe|pub|food_court|marketplace|fountain|public_bath|sauna|stripclub|brothel)$\"](area.searchArea);\n",
    "\n",
    "      node[\"shop\"~\"^(mall|department_store|books|gift|souvenir|art|antiques|craft|boutique|jewelry|leather|music|shoes|toys|video)$\"](area.searchArea);\n",
    "      way[\"shop\"~\"^(mall|department_store|books|gift|souvenir|art|antiques|craft|boutique|jewelry|leather|music|shoes|toys|video)$\"](area.searchArea);\n",
    "    );\n",
    "    out body;\n",
    "    >;\n",
    "    out skel qt;\n",
    "    \"\"\"\n",
    "\n",
    "    amenities_to_include = [\n",
    "        \"arts_centre\", \"theatre\", \"cinema\", \"museum\", \"library\", \"nightclub\", \"bar\",\n",
    "        \"restaurant\", \"cafe\", \"pub\", \"food_court\", \"marketplace\", \"fountain\",\n",
    "        \"public_bath\", \"sauna\", \"stripclub\", \"brothel\", \"casino\", \"ferry_terminal\",\n",
    "        \"attraction\", \"theme_park\", \"water_park\", \"zoo\", \"aquarium\", \"planetarium\",\n",
    "        \"gallery\", \"viewpoint\", \"observatory\"\n",
    "    ]\n",
    "    shops_to_include = [\n",
    "        \"mall\", \"department_store\", \"books\", \"gift\", \"souvenir\", \"art\", \"antiques\",\n",
    "        \"craft\", \"boutique\", \"jewelry\", \"leather\", \"music\", \"shoes\", \"toys\", \"video\",\n",
    "        \"kiosk\", \"convenience\"\n",
    "    ]\n",
    "\n",
    "    amenity_regex = \"|\".join(amenities_to_include)\n",
    "    shop_regex = \"|\".join(shops_to_include)\n",
    "\n",
    "    overpass_query_filtered = f\"\"\"\n",
    "    [out:json][timeout:180];\n",
    "    area[name=\"{city_name}\"]->.searchArea;\n",
    "    (\n",
    "      node[\"tourism\"](area.searchArea);\n",
    "      way[\"tourism\"](area.searchArea);\n",
    "      node[\"leisure\"](area.searchArea);\n",
    "      way[\"leisure\"](area.searchArea);\n",
    "      node[\"historic\"](area.searchArea);\n",
    "      way[\"historic\"](area.searchArea);\n",
    "\n",
    "      node[\"place\"~\"square|fountain\"](area.searchArea);\n",
    "      way[\"place\"~\"square|fountain\"](area.searchArea);\n",
    "      node[\"natural\"~\"park|wood|garden|beach|peak|water\"](area.searchArea);\n",
    "      way[\"natural\"~\"park|wood|garden|beach|peak|water\"](area.searchArea);\n",
    "\n",
    "      node[\"amenity\"~\"^{amenity_regex}$\"](area.searchArea);\n",
    "      way[\"amenity\"~\"^{amenity_regex}$\"](area.searchArea);\n",
    "\n",
    "      node[\"shop\"~\"^{shop_regex}$\"](area.searchArea);\n",
    "      way[\"shop\"~\"^{shop_regex}$\"](area.searchArea);\n",
    "    );\n",
    "    out body;\n",
    "    >;\n",
    "    out skel qt;\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"üöÄ –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ Overpass API –¥–ª—è {city_name}...\")\n",
    "    try:\n",
    "        response = requests.get(OVERPASS_URL, params={'data': overpass_query_filtered}, timeout=180)\n",
    "        print(f\"‚úÖ –ó–∞–ø—Ä–æ—Å –¥–ª—è {city_name} –≤—ã–ø–æ–ª–Ω–µ–Ω —Å –∫–æ–¥–æ–º: {response.status_code}\")\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "        else:\n",
    "            print(f\"–û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è {city_name}: {response.text}\")\n",
    "            data = {'elements': []}\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(f\"–¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ Overpass API –¥–ª—è {city_name}\")\n",
    "        data = {'elements': []}\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ Overpass API –¥–ª—è {city_name}: {e}\")\n",
    "        data = {'elements': []}\n",
    "\n",
    "\n",
    "    current_city_pois = []\n",
    "    print(f\"‚öôÔ∏è –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É {len(data['elements'])} –æ–±—ä–µ–∫—Ç–æ–≤ –¥–ª—è {city_name}...\")\n",
    "\n",
    "    for element in tqdm(data['elements'], desc=f\"–û–±—Ä–∞–±–æ—Ç–∫–∞ POI –≤ {city_name}\"):\n",
    "        if 'tags' in element:\n",
    "            tags = element['tags']\n",
    "\n",
    "            if 'name' not in tags:\n",
    "                continue\n",
    "\n",
    "            lat, lon = (0, 0)\n",
    "            if element['type'] == 'node':\n",
    "                lat = element.get('lat')\n",
    "                lon = element.get('lon')\n",
    "            elif 'center' in element:\n",
    "                lat = element['center'].get('lat')\n",
    "                lon = element['center'].get('lon')\n",
    "\n",
    "            if lat == 0 and lon == 0:\n",
    "                continue\n",
    "\n",
    "            text_description = create_text_description(tags)\n",
    "\n",
    "            current_city_pois.append({\n",
    "                'id': element['id'],\n",
    "                'type': element['type'],\n",
    "                'lat': lat,\n",
    "                'lon': lon,\n",
    "                'name': tags.get('name'),\n",
    "                'city': city_name,\n",
    "                'text_description': text_description,\n",
    "                'tags': tags\n",
    "            })\n",
    "\n",
    "        time.sleep(0.005) # 5 –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥\n",
    "\n",
    "    print(f\"üëç –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(current_city_pois)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤ —Å –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏ –¥–ª—è {city_name}.\")\n",
    "\n",
    "    all_processed_pois.extend(current_city_pois)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cities_to_process = [\"–ö–∞–∑–∞–Ω—å\", \"–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥\", \"–ú–æ—Å–∫–≤–∞\"]\n",
    "\n",
    "    for city in cities_to_process:\n",
    "        process_city_pois(city)\n",
    "        time.sleep(10)\n",
    "\n",
    "    print(\"\\n--- –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö –≥–æ—Ä–æ–¥–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–∞! ---\")\n",
    "    print(f\"–í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ {len(all_processed_pois)} POI.\")\n",
    "\n",
    "    if all_processed_pois:\n",
    "        final_df = pd.DataFrame(all_processed_pois)\n",
    "        final_df.drop_duplicates(subset=['id'], inplace=True)\n",
    "        print(f\"–í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö POI –ø–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {len(final_df)}\")\n",
    "\n",
    "        output_filename = 'Dataset/poi_dataset_russia_filtered_enriched.csv'\n",
    "        final_df.to_csv(output_filename, index=False)\n",
    "        print(f\"üíæ –û–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ '{output_filename}'\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è –û–±—â–∏–π –¥–∞—Ç–∞—Å–µ—Ç –ø—É—Å—Ç. –í–æ–∑–º–æ–∂–Ω–æ, –≤–æ–∑–Ω–∏–∫–ª–∏ –æ—à–∏–±–∫–∏ –ø—Ä–∏ —Å–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö.\")"
   ],
   "id": "ee5714ef775ca054",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –≥–æ—Ä–æ–¥–∞: –ö–∞–∑–∞–Ω—å ---\n",
      "üöÄ –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ Overpass API –¥–ª—è –ö–∞–∑–∞–Ω—å...\n",
      "‚úÖ –ó–∞–ø—Ä–æ—Å –¥–ª—è –ö–∞–∑–∞–Ω—å –≤—ã–ø–æ–ª–Ω–µ–Ω —Å –∫–æ–¥–æ–º: 200\n",
      "‚öôÔ∏è –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É 73622 –æ–±—ä–µ–∫—Ç–æ–≤ –¥–ª—è –ö–∞–∑–∞–Ω—å...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "–û–±—Ä–∞–±–æ—Ç–∫–∞ POI –≤ –ö–∞–∑–∞–Ω—å: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 73622/73622 [06:29<00:00, 189.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üëç –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 3060 —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤ —Å –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏ –¥–ª—è –ö–∞–∑–∞–Ω—å.\n",
      "\n",
      "--- –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –≥–æ—Ä–æ–¥–∞: –°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥ ---\n",
      "üöÄ –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ Overpass API –¥–ª—è –°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥...\n",
      "‚úÖ –ó–∞–ø—Ä–æ—Å –¥–ª—è –°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥ –≤—ã–ø–æ–ª–Ω–µ–Ω —Å –∫–æ–¥–æ–º: 200\n",
      "‚öôÔ∏è –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É 413806 –æ–±—ä–µ–∫—Ç–æ–≤ –¥–ª—è –°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "–û–±—Ä–∞–±–æ—Ç–∫–∞ POI –≤ –°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 413806/413806 [37:42<00:00, 182.87it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üëç –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 16858 —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤ —Å –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏ –¥–ª—è –°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥.\n",
      "\n",
      "--- –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –≥–æ—Ä–æ–¥–∞: –ú–æ—Å–∫–≤–∞ ---\n",
      "üöÄ –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ Overpass API –¥–ª—è –ú–æ—Å–∫–≤–∞...\n",
      "‚úÖ –ó–∞–ø—Ä–æ—Å –¥–ª—è –ú–æ—Å–∫–≤–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω —Å –∫–æ–¥–æ–º: 200\n",
      "‚öôÔ∏è –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É 608685 –æ–±—ä–µ–∫—Ç–æ–≤ –¥–ª—è –ú–æ—Å–∫–≤–∞...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "–û–±—Ä–∞–±–æ—Ç–∫–∞ POI –≤ –ú–æ—Å–∫–≤–∞: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 608685/608685 [54:07<00:00, 187.45it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üëç –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 21178 —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤ —Å –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏ –¥–ª—è –ú–æ—Å–∫–≤–∞.\n",
      "\n",
      "--- –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö –≥–æ—Ä–æ–¥–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–∞! ---\n",
      "–í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ 41096 POI.\n",
      "–í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö POI –ø–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: 41087\n",
      "üíæ –û–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ 'poi_dataset_russia_filtered_enriched.csv'\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T12:15:10.219522Z",
     "start_time": "2025-06-17T11:58:56.662281Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "OVERPASS_URL = \"http://overpass-api.de/api/interpreter\"\n",
    "WIKIPEDIA_API_URL = \"https://ru.wikipedia.org/w/api.php\"\n",
    "WIKIDATA_API_URL = \"https://www.wikidata.org/w/api.php\"\n",
    "\n",
    "def get_wikipedia_summary(title):\n",
    "    try:\n",
    "        params = {\n",
    "            \"action\": \"query\", \"format\": \"json\", \"titles\": title,\n",
    "            \"prop\": \"extracts\", \"exintro\": True, \"explaintext\": True, \"redirects\": 1\n",
    "        }\n",
    "        response = requests.get(WIKIPEDIA_API_URL, params=params, timeout=10)\n",
    "        data = response.json()\n",
    "        page_id = next(iter(data['query']['pages']))\n",
    "        if page_id != '-1':\n",
    "            return data['query']['pages'][page_id].get('extract', '')\n",
    "        return \"\"\n",
    "    except requests.exceptions.Timeout:\n",
    "        return \"\"\n",
    "    except requests.exceptions.RequestException:\n",
    "        return \"\"\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def get_wikidata_description(qid):\n",
    "    try:\n",
    "        params = {\n",
    "            \"action\": \"wbgetentities\", \"format\": \"json\", \"ids\": qid,\n",
    "            \"props\": \"descriptions\", \"languages\": \"ru|en\"\n",
    "        }\n",
    "        response = requests.get(WIKIDATA_API_URL, params=params, timeout=10)\n",
    "        data = response.json()\n",
    "        entity = data['entities'].get(qid)\n",
    "        if entity and 'descriptions' in entity:\n",
    "            if 'ru' in entity['descriptions']:\n",
    "                return entity['descriptions']['ru']['value']\n",
    "            elif 'en' in entity['descriptions']:\n",
    "                return entity['descriptions']['en']['value']\n",
    "        return \"\"\n",
    "    except requests.exceptions.Timeout:\n",
    "        return \"\"\n",
    "    except requests.exceptions.RequestException:\n",
    "        return \"\"\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def create_text_description(tags):\n",
    "    description_parts = []\n",
    "    if 'name' in tags:\n",
    "        description_parts.append(tags['name'])\n",
    "\n",
    "    tag_translations = {\n",
    "        'amenity': '–¢–∏–ø', 'shop': '–ú–∞–≥–∞–∑–∏–Ω', 'tourism': '–¢—É—Ä–∏–∑–º',\n",
    "        'leisure': '–î–æ—Å—É–≥', 'historic': '–ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–π –æ–±—ä–µ–∫—Ç', 'cuisine': '–ö—É—Ö–Ω—è'\n",
    "    }\n",
    "    for key, name in tag_translations.items():\n",
    "        if key in tags:\n",
    "            description_parts.append(f\"{name}: {tags[key].replace('_', ' ')}\")\n",
    "\n",
    "    if 'opening_hours' in tags: description_parts.append(f\"–ß–∞—Å—ã —Ä–∞–±–æ—Ç—ã: {tags['opening_hours']}\")\n",
    "    if 'wheelchair' in tags and tags['wheelchair'] == 'yes': description_parts.append(\"–î–æ—Å—Ç—É–ø–Ω–æ –¥–ª—è –∏–Ω–≤–∞–ª–∏–¥–Ω—ã—Ö –∫–æ–ª—è—Å–æ–∫\")\n",
    "    if 'internet_access' in tags and tags['internet_access'] == 'yes': description_parts.append(\"–ï—Å—Ç—å –¥–æ—Å—Ç—É–ø –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç\")\n",
    "    if 'phone' in tags: description_parts.append(f\"–¢–µ–ª–µ—Ñ–æ–Ω: {tags['phone']}\")\n",
    "    if 'website' in tags: description_parts.append(f\"–í–µ–±—Å–∞–π—Ç: {tags['website']}\")\n",
    "    if 'description' in tags: description_parts.append(f\"–û–ø–∏—Å–∞–Ω–∏–µ: {tags['description']}\")\n",
    "\n",
    "    extra_description = \"\"\n",
    "    if 'wikipedia' in tags:\n",
    "        parts = tags['wikipedia'].split(':')\n",
    "        if len(parts) >= 2:\n",
    "            lang = parts[0]\n",
    "            title = parts[-1]\n",
    "            if lang == 'ru':\n",
    "                wiki_summary = get_wikipedia_summary(title)\n",
    "                if wiki_summary: extra_description = wiki_summary\n",
    "    elif 'wikidata' in tags:\n",
    "        wikidata_id = tags['wikidata']\n",
    "        wiki_description = get_wikidata_description(wikidata_id)\n",
    "        if wiki_description: extra_description = wiki_description\n",
    "\n",
    "    if extra_description: description_parts.append(extra_description)\n",
    "    return \". \".join(description_parts)\n",
    "\n",
    "def process_city_pois(city_name):\n",
    "    print(f\"\\n--- –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –≥–æ—Ä–æ–¥–∞: {city_name} ---\")\n",
    "\n",
    "    amenities_to_include = [\n",
    "        \"arts_centre\", \"theatre\", \"cinema\", \"museum\", \"library\", \"nightclub\", \"bar\",\n",
    "        \"restaurant\", \"cafe\", \"pub\", \"food_court\", \"marketplace\", \"fountain\",\n",
    "        \"public_bath\", \"sauna\", \"casino\", \"ferry_terminal\", \"attraction\",\n",
    "        \"theme_park\", \"water_park\", \"zoo\", \"aquarium\", \"planetarium\", \"gallery\",\n",
    "        \"viewpoint\", \"observatory\", \"place_of_worship\", \"community_centre\", \"social_facility\"\n",
    "    ]\n",
    "    shops_to_include = [\n",
    "        \"mall\", \"department_store\", \"books\", \"gift\", \"souvenir\", \"art\", \"antiques\",\n",
    "        \"craft\", \"boutique\", \"jewelry\", \"leather\", \"music\", \"shoes\", \"toys\", \"video\",\n",
    "        \"convenience\", \"supermarket\", \"bakery\", \"beverages\", \"confectionery\", \"deli\",\n",
    "        \"farm\", \"greengrocer\", \"ice_cream\", \"pastry\", \"wine\", \"stationery\", \"sports\",\n",
    "        \"fashion\", \"perfumery\"\n",
    "    ]\n",
    "\n",
    "    amenity_filter = \"|\".join(amenities_to_include)\n",
    "    shop_filter = \"|\".join(shops_to_include)\n",
    "\n",
    "    overpass_query_strict = f\"\"\"\n",
    "    [out:json][timeout:180];\n",
    "    area[name=\"{city_name}\"]->.searchArea;\n",
    "    (\n",
    "      node[\"tourism\"](area.searchArea);\n",
    "      way[\"tourism\"](area.searchArea);\n",
    "      relation[\"tourism\"](area.searchArea);\n",
    "\n",
    "      node[\"leisure\"](area.searchArea);\n",
    "      way[\"leisure\"](area.searchArea);\n",
    "      relation[\"leisure\"](area.searchArea);\n",
    "\n",
    "      node[\"historic\"](area.searchArea);\n",
    "      way[\"historic\"](area.searchArea);\n",
    "      relation[\"historic\"](area.searchArea);\n",
    "\n",
    "      node[\"place\"~\"square|fountain\"](area.searchArea);\n",
    "      way[\"place\"~\"square|fountain\"](area.searchArea);\n",
    "\n",
    "      node[\"natural\"~\"park|wood|garden|beach|peak|water|forest|island|ridge|valley|volcano|wetland|glacier\"](area.searchArea);\n",
    "      way[\"natural\"~\"park|wood|garden|beach|peak|water|forest|island|ridge|valley|volcano|wetland|glacier\"](area.searchArea);\n",
    "      relation[\"natural\"~\"park|wood|garden|beach|peak|water|forest|island|ridge|valley|volcano|wetland|glacier\"](area.searchArea);\n",
    "\n",
    "      node[\"amenity\"~\"^{amenity_filter}$\"](area.searchArea);\n",
    "      way[\"amenity\"~\"^{amenity_filter}$\"](area.searchArea);\n",
    "\n",
    "      node[\"shop\"~\"^{shop_filter}$\"](area.searchArea);\n",
    "      way[\"shop\"~\"^{shop_filter}$\"](area.searchArea);\n",
    "\n",
    "      node[\"landuse\"~\"forest|park|recreation_ground|village_green\"](area.searchArea);\n",
    "      way[\"landuse\"~\"forest|park|recreation_ground|village_green\"](area.searchArea);\n",
    "    );\n",
    "    out body;\n",
    "    >;\n",
    "    out skel qt;\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"üöÄ –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å –∫ Overpass API –¥–ª—è {city_name}...\")\n",
    "    try:\n",
    "        response = requests.get(OVERPASS_URL, params={'data': overpass_query_strict}, timeout=180)\n",
    "        print(f\"‚úÖ –ó–∞–ø—Ä–æ—Å –¥–ª—è {city_name} –≤—ã–ø–æ–ª–Ω–µ–Ω —Å –∫–æ–¥–æ–º: {response.status_code}\")\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "        else:\n",
    "            print(f\"–û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è {city_name}: {response.text}\")\n",
    "            data = {'elements': []}\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(f\"–¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ Overpass API –¥–ª—è {city_name}. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —É–≤–µ–ª–∏—á–∏—Ç—å timeout.\")\n",
    "        data = {'elements': []}\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ Overpass API –¥–ª—è {city_name}: {e}\")\n",
    "        data = {'elements': []}\n",
    "\n",
    "    current_city_pois = []\n",
    "    print(f\"‚öôÔ∏è –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É {len(data['elements'])} –æ–±—ä–µ–∫—Ç–æ–≤, –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö –∏–∑ Overpass –¥–ª—è {city_name}...\")\n",
    "\n",
    "    for element in tqdm(data['elements'], desc=f\"–û–±—Ä–∞–±–æ—Ç–∫–∞ –∏ –æ–±–æ–≥–∞—â–µ–Ω–∏–µ POI –≤ {city_name}\"):\n",
    "        if 'tags' in element:\n",
    "            tags = element['tags']\n",
    "\n",
    "            if 'name' not in tags:\n",
    "                continue\n",
    "\n",
    "            lat, lon = (0, 0)\n",
    "            if element['type'] == 'node':\n",
    "                lat = element.get('lat')\n",
    "                lon = element.get('lon')\n",
    "            elif element['type'] == 'way' and 'center' in element:\n",
    "                lat = element['center'].get('lat')\n",
    "                lon = element['center'].get('lon')\n",
    "            elif element['type'] == 'relation' and 'center' in element:\n",
    "                lat = element['center'].get('lat')\n",
    "                lon = element['center'].get('lon')\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            if lat == 0 and lon == 0:\n",
    "                continue\n",
    "\n",
    "            text_description = create_text_description(tags)\n",
    "\n",
    "            current_city_pois.append({\n",
    "                'id': element['id'],\n",
    "                'type': element['type'],\n",
    "                'lat': lat,\n",
    "                'lon': lon,\n",
    "                'name': tags.get('name'),\n",
    "                'city': city_name,\n",
    "                'text_description': text_description,\n",
    "                'tags': tags\n",
    "            })\n",
    "\n",
    "        time.sleep(0.005)\n",
    "\n",
    "    print(f\"üëç –í—Å–µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∏ –¥–æ–±–∞–≤–ª–µ–Ω–æ –≤ —Å–ø–∏—Å–æ–∫: {len(current_city_pois)} POI –¥–ª—è {city_name}.\")\n",
    "\n",
    "    return current_city_pois\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    output_filename = 'Dataset/poi_dataset_russia_filtered_enriched.csv'\n",
    "\n",
    "    if os.path.exists(output_filename):\n",
    "        print(f\"–ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ '{output_filename}'...\")\n",
    "        existing_df = pd.read_csv(output_filename)\n",
    "        print(f\"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(existing_df)} POI –∏–∑ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ —Ñ–∞–π–ª–∞.\")\n",
    "    else:\n",
    "        print(f\"–§–∞–π–ª '{output_filename}' –Ω–µ –Ω–∞–π–¥–µ–Ω. –°–æ–∑–¥–∞–¥–∏–º –Ω–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç.\")\n",
    "        existing_df = pd.DataFrame()\n",
    "\n",
    "    cities_to_add = [\"–ï–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥\"]\n",
    "\n",
    "    newly_collected_pois = []\n",
    "    for city in cities_to_add:\n",
    "        city_data = process_city_pois(city)\n",
    "        newly_collected_pois.extend(city_data)\n",
    "        time.sleep(10)\n",
    "\n",
    "    print(\"\\n--- –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –Ω–æ–≤—ã—Ö –≥–æ—Ä–æ–¥–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω! ---\")\n",
    "    print(f\"–í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ {len(newly_collected_pois)} –Ω–æ–≤—ã—Ö POI.\")\n",
    "\n",
    "    combined_pois = pd.concat([existing_df, pd.DataFrame(newly_collected_pois)], ignore_index=True)\n",
    "\n",
    "    print(f\"–í—Å–µ–≥–æ POI –¥–æ —É–¥–∞–ª–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {len(combined_pois)}\")\n",
    "    combined_pois.dropna(subset=['name', 'lat', 'lon'], inplace=True)\n",
    "    combined_pois.drop_duplicates(subset=['id'], inplace=True)\n",
    "\n",
    "    print(f\"–ò—Ç–æ–≥–æ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∏ –ø–æ–ª–Ω—ã—Ö POI: {len(combined_pois)}\")\n",
    "\n",
    "    combined_pois.to_csv(output_filename, index=False)\n",
    "    print(f\"üíæ –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ '{output_filename}'\")"
   ],
   "id": "b39d2a53a961737b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ 'poi_dataset_russia_filtered_enriched.csv'...\n",
      "–ó–∞–≥—Ä—É–∂–µ–Ω–æ 46354 POI –∏–∑ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ —Ñ–∞–π–ª–∞.\n",
      "\n",
      "--- –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –≥–æ—Ä–æ–¥–∞: –ï–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥ ---\n",
      "üöÄ –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å –∫ Overpass API –¥–ª—è –ï–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥...\n",
      "‚úÖ –ó–∞–ø—Ä–æ—Å –¥–ª—è –ï–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥ –≤—ã–ø–æ–ª–Ω–µ–Ω —Å –∫–æ–¥–æ–º: 200\n",
      "‚öôÔ∏è –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É 173809 –æ–±—ä–µ–∫—Ç–æ–≤, –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö –∏–∑ Overpass –¥–ª—è –ï–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "–û–±—Ä–∞–±–æ—Ç–∫–∞ –∏ –æ–±–æ–≥–∞—â–µ–Ω–∏–µ POI –≤ –ï–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 173809/173809 [15:47<00:00, 183.35it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üëç –í—Å–µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∏ –¥–æ–±–∞–≤–ª–µ–Ω–æ –≤ —Å–ø–∏—Å–æ–∫: 4109 POI –¥–ª—è –ï–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥.\n",
      "\n",
      "--- –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –Ω–æ–≤—ã—Ö –≥–æ—Ä–æ–¥–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω! ---\n",
      "–í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ 4109 –Ω–æ–≤—ã—Ö POI.\n",
      "–í—Å–µ–≥–æ POI –¥–æ —É–¥–∞–ª–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: 50463\n",
      "–ò—Ç–æ–≥–æ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∏ –ø–æ–ª–Ω—ã—Ö POI: 50463\n",
      "üíæ –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ 'poi_dataset_russia_filtered_enriched.csv'\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T13:04:24.841441Z",
     "start_time": "2025-06-22T13:04:09.440311Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from dateutil import parser as dateparser\n",
    "\n",
    "df = pd.read_csv('Dataset/poi_dataset_russia_filtered_enriched.csv', dtype=str)\n",
    "df = df.drop_duplicates('id')\n",
    "\n",
    "tags_df = (\n",
    "    df['tags']\n",
    "    .apply(lambda x: ast.literal_eval(x) if pd.notnull(x) else {})\n",
    "    .apply(pd.Series)\n",
    ")\n",
    "tags_df = tags_df.rename(columns={c: c.replace(':','_').lower() for c in tags_df.columns})\n",
    "tags_df = tags_df.drop(columns=['name'], errors=True)\n",
    "\n",
    "tags_df['address'] = (\n",
    "    tags_df[['addr_street','addr_housenumber','addr_floor']]\n",
    "    .fillna('')\n",
    "    .agg(' '.join, axis=1)\n",
    "    .str.strip()\n",
    ")\n",
    "\n",
    "def normalize_phone(x):\n",
    "    if not isinstance(x, str) or not x.strip():\n",
    "        return None\n",
    "    digits = re.sub(r'\\D','',x)\n",
    "    if len(digits) == 10:\n",
    "        return '+7' + digits\n",
    "    if len(digits) == 11 and digits.startswith('8'):\n",
    "        return '+7' + digits[1:]\n",
    "    if len(digits) == 11 and digits.startswith('7'):\n",
    "        return '+' + digits\n",
    "    return None\n",
    "\n",
    "tags_df['phone_e164'] = (\n",
    "    tags_df.get('contact_phone','')\n",
    "    .combine_first(tags_df.get('phone',''))\n",
    "    .apply(normalize_phone)\n",
    ")\n",
    "\n",
    "def normalize_url(x):\n",
    "    if pd.isnull(x) or not x:\n",
    "        return None\n",
    "    return x if re.match(r'^https?://', x) else 'http://' + x\n",
    "\n",
    "tags_df['website'] = tags_df.get('contact_website','').apply(normalize_url)\n",
    "\n",
    "tags_df['check_date'] = tags_df.get('check_date','').apply(\n",
    "    lambda x: dateparser.parse(x).date().isoformat() if pd.notnull(x) and x else None\n",
    ")\n",
    "\n",
    "tags_df['opening_hours'] = (\n",
    "    tags_df.get('opening_hours','')\n",
    "    .str.replace(r'\\s*;\\s*','; ', regex=True)\n",
    "    .str.strip()\n",
    ")\n",
    "\n",
    "def normalize_wheelchair(x):\n",
    "    if pd.isnull(x):\n",
    "        return None\n",
    "    val = x.lower()\n",
    "    if val in ('yes','true'):\n",
    "        return True\n",
    "    if val in ('no','false'):\n",
    "        return False\n",
    "    return val\n",
    "\n",
    "tags_df['wheelchair'] = tags_df.get('wheelchair','').apply(normalize_wheelchair)\n",
    "\n",
    "category_cols = ['amenity','shop','tourism','historic','cuisine','memorial']\n",
    "tags_df['categories'] = tags_df[category_cols].apply(\n",
    "    lambda row: [v for v in row if pd.notnull(v) and v!=''], axis=1\n",
    ")\n",
    "\n",
    "df_clean = pd.concat([df.drop(columns=['tags']), tags_df], axis=1)\n",
    "\n",
    "df_clean['name'] = df_clean['name'].str.strip().str.title()\n",
    "if 'city' in df_clean.columns:\n",
    "    df_clean['city'] = df_clean['city'].str.strip().str.title()\n",
    "\n",
    "def clean_text(x):\n",
    "    if pd.isnull(x):\n",
    "        return ''\n",
    "    raw = BeautifulSoup(x, 'html.parser').get_text()\n",
    "    return re.sub(r'\\s+', ' ', raw).strip()\n",
    "\n",
    "df_clean['text_description'] = df_clean['text_description'].apply(clean_text)\n",
    "\n",
    "output_path = 'poi_dataset_cleaned.csv'\n",
    "df_clean.to_csv(output_path, index=False)\n",
    "print(f\"Cleaned dataset saved to ./{output_path}\")\n"
   ],
   "id": "a16bcfd91bfc8dce",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset saved to ./poi_dataset_cleaned.csv\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T13:55:07.944413Z",
     "start_time": "2025-06-22T13:54:56.059443Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline,\n",
    ")\n",
    "\n",
    "MODEL_NAME = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\",\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device_map=\"auto\",\n",
    "    max_new_tokens=200,\n",
    "    do_sample=False\n",
    ")\n",
    "\n",
    "def enrich_poi_llama(description: str) -> dict:\n",
    "    prompt = f\"\"\"\n",
    "You are a travel assistant. Given the following description in Russian, output a JSON object with exactly two keys:\n",
    "  \"summary\": one-sentence thematic summary like \"—Ä–æ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ –º–µ—Å—Ç–æ –¥–ª—è –≤–µ—á–µ—Ä–Ω–µ–π –ø—Ä–æ–≥—É–ª–∫–∏\",\n",
    "  \"themes\": list of 2‚Äì4 keywords describing main place types or themes (e.g. [\"–º—É–∑–µ–π\",\"–ø–∞—Ä–∫\"]).\n",
    "\n",
    "Description:\n",
    "\\\"\\\"\\\"{description.strip()}\\\"\\\"\\\"\n",
    "\n",
    "Respond ONLY with the JSON object.\n",
    "\"\"\"\n",
    "    out = generator(prompt)[0][\"generated_text\"]\n",
    "    json_str = out[out.find(\"{\"): out.rfind(\"}\")+1]\n",
    "    return json.loads(json_str)\n",
    "\n",
    "df = pd.read_csv(\"poi_dataset_cleaned.csv\", dtype=str)\n",
    "summaries, themes = [], []\n",
    "\n",
    "for desc in tqdm(df[\"text_description\"], desc=\"Enriching with Llama2-7B 4bit\"):\n",
    "    if not isinstance(desc, str) or not desc.strip():\n",
    "        summaries.append(None)\n",
    "        themes.append([])\n",
    "        continue\n",
    "    try:\n",
    "        enriched = enrich_poi_llama(desc)\n",
    "        summaries.append(enriched.get(\"summary\"))\n",
    "        themes.append(enriched.get(\"themes\", []))\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "        summaries.append(None)\n",
    "        themes.append([])\n",
    "\n",
    "df[\"summary\"] = summaries\n",
    "df[\"themes\"]  = themes\n",
    "df.to_csv(\"poi_dataset_enriched_local.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(\"Done! Saved to poi_dataset_enriched_local.csv\")\n"
   ],
   "id": "3a65cbfb2fc79e9e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emil1\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[9]\u001B[39m\u001B[32m, line 13\u001B[39m\n\u001B[32m     11\u001B[39m \u001B[38;5;66;03m# --- 1. –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è 4-–±–∏—Ç–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ ---\u001B[39;00m\n\u001B[32m     12\u001B[39m MODEL_NAME = \u001B[33m\"\u001B[39m\u001B[33mmeta-llama/Llama-2-7b-chat-hf\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m---> \u001B[39m\u001B[32m13\u001B[39m bnb_config = \u001B[43mBitsAndBytesConfig\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     14\u001B[39m \u001B[43m    \u001B[49m\u001B[43mload_in_4bit\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m     15\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbnb_4bit_quant_type\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mnf4\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     16\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbnb_4bit_use_double_quant\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\n\u001B[32m     17\u001B[39m \u001B[43m)\u001B[49m\n\u001B[32m     19\u001B[39m \u001B[38;5;66;03m# --- 2. –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∏ –º–æ–¥–µ–ª–∏ —Å –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ–º ---\u001B[39;00m\n\u001B[32m     20\u001B[39m tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\utils\\quantization_config.py:484\u001B[39m, in \u001B[36mBitsAndBytesConfig.__init__\u001B[39m\u001B[34m(self, load_in_8bit, load_in_4bit, llm_int8_threshold, llm_int8_skip_modules, llm_int8_enable_fp32_cpu_offload, llm_int8_has_fp16_weight, bnb_4bit_compute_dtype, bnb_4bit_quant_type, bnb_4bit_use_double_quant, bnb_4bit_quant_storage, **kwargs)\u001B[39m\n\u001B[32m    481\u001B[39m \u001B[38;5;28mself\u001B[39m.bnb_4bit_use_double_quant = bnb_4bit_use_double_quant\n\u001B[32m    483\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m bnb_4bit_compute_dtype \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m484\u001B[39m     \u001B[38;5;28mself\u001B[39m.bnb_4bit_compute_dtype = \u001B[43mtorch\u001B[49m.float32\n\u001B[32m    485\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(bnb_4bit_compute_dtype, \u001B[38;5;28mstr\u001B[39m):\n\u001B[32m    486\u001B[39m     \u001B[38;5;28mself\u001B[39m.bnb_4bit_compute_dtype = \u001B[38;5;28mgetattr\u001B[39m(torch, bnb_4bit_compute_dtype)\n",
      "\u001B[31mNameError\u001B[39m: name 'torch' is not defined"
     ]
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

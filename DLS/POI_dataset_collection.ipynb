{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-17T09:34:12.422066Z",
     "start_time": "2025-06-17T09:34:09.674748Z"
    }
   },
   "source": [
    "!pip install requests pandas\n",
    "!pip install tqdm"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\emil1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\emil1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\emil1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\emil1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\emil1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\emil1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (2025.4.26)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\emil1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\emil1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\emil1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\emil1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\emil1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\emil1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\emil1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tqdm) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T11:14:04.062764Z",
     "start_time": "2025-06-17T09:34:12.434651Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "OVERPASS_URL = \"http://overpass-api.de/api/interpreter\"\n",
    "WIKIPEDIA_API_URL = \"https://ru.wikipedia.org/w/api.php\"\n",
    "WIKIDATA_API_URL = \"https://www.wikidata.org/w/api.php\"\n",
    "\n",
    "all_processed_pois = []\n",
    "\n",
    "def get_wikipedia_summary(title):\n",
    "    \"\"\"–ü–æ–ª—É—á–∞–µ—Ç –∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –∏–∑ –í–∏–∫–∏–ø–µ–¥–∏–∏ –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é —Å—Ç–∞—Ç—å–∏.\"\"\"\n",
    "    try:\n",
    "        params = {\n",
    "            \"action\": \"query\",\n",
    "            \"format\": \"json\",\n",
    "            \"titles\": title,\n",
    "            \"prop\": \"extracts\",\n",
    "            \"exintro\": True,\n",
    "            \"explaintext\": True,\n",
    "            \"redirects\": 1\n",
    "        }\n",
    "        response = requests.get(WIKIPEDIA_API_URL, params=params, timeout=10)\n",
    "        data = response.json()\n",
    "        page_id = next(iter(data['query']['pages']))\n",
    "        if page_id != '-1':\n",
    "            return data['query']['pages'][page_id].get('extract', '')\n",
    "        return \"\"\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(f\"–¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ –í–∏–∫–∏–ø–µ–¥–∏–∏ –¥–ª—è {title}\")\n",
    "        return \"\"\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        return \"\"\n",
    "\n",
    "def get_wikidata_description(qid):\n",
    "    try:\n",
    "        params = {\n",
    "            \"action\": \"wbgetentities\",\n",
    "            \"format\": \"json\",\n",
    "            \"ids\": qid,\n",
    "            \"props\": \"descriptions\",\n",
    "            \"languages\": \"ru|en\"\n",
    "        }\n",
    "        response = requests.get(WIKIDATA_API_URL, params=params, timeout=10)\n",
    "        data = response.json()\n",
    "        entity = data['entities'].get(qid)\n",
    "        if entity and 'descriptions' in entity:\n",
    "            if 'ru' in entity['descriptions']:\n",
    "                return entity['descriptions']['ru']['value']\n",
    "            elif 'en' in entity['descriptions']:\n",
    "                return entity['descriptions']['en']['value']\n",
    "        return \"\"\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(f\"–¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ Wikidata –¥–ª—è {qid}\")\n",
    "        return \"\"\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        return \"\"\n",
    "\n",
    "def create_text_description(tags):\n",
    "    description_parts = []\n",
    "\n",
    "    if 'name' in tags:\n",
    "        description_parts.append(tags['name'])\n",
    "\n",
    "    tag_translations = {\n",
    "        'amenity': '–¢–∏–ø',\n",
    "        'shop': '–ú–∞–≥–∞–∑–∏–Ω',\n",
    "        'tourism': '–¢—É—Ä–∏–∑–º',\n",
    "        'leisure': '–î–æ—Å—É–≥',\n",
    "        'historic': '–ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–π –æ–±—ä–µ–∫—Ç',\n",
    "        'cuisine': '–ö—É—Ö–Ω—è'\n",
    "    }\n",
    "\n",
    "    for key, name in tag_translations.items():\n",
    "        if key in tags:\n",
    "            description_parts.append(f\"{name}: {tags[key].replace('_', ' ')}\")\n",
    "\n",
    "    if 'opening_hours' in tags:\n",
    "        description_parts.append(f\"–ß–∞—Å—ã —Ä–∞–±–æ—Ç—ã: {tags['opening_hours']}\")\n",
    "    if 'wheelchair' in tags and tags['wheelchair'] == 'yes':\n",
    "        description_parts.append(\"–î–æ—Å—Ç—É–ø–Ω–æ –¥–ª—è –∏–Ω–≤–∞–ª–∏–¥–Ω—ã—Ö –∫–æ–ª—è—Å–æ–∫\")\n",
    "    if 'internet_access' in tags and tags['internet_access'] == 'yes':\n",
    "        description_parts.append(\"–ï—Å—Ç—å –¥–æ—Å—Ç—É–ø –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç\")\n",
    "    if 'phone' in tags:\n",
    "        description_parts.append(f\"–¢–µ–ª–µ—Ñ–æ–Ω: {tags['phone']}\")\n",
    "    if 'website' in tags:\n",
    "        description_parts.append(f\"–í–µ–±—Å–∞–π—Ç: {tags['website']}\")\n",
    "\n",
    "    if 'description' in tags:\n",
    "        description_parts.append(f\"–û–ø–∏—Å–∞–Ω–∏–µ: {tags['description']}\")\n",
    "\n",
    "    extra_description = \"\"\n",
    "    if 'wikipedia' in tags:\n",
    "        parts = tags['wikipedia'].split(':')\n",
    "        if len(parts) >= 2:\n",
    "            lang = parts[0]\n",
    "            title = parts[-1]\n",
    "            if lang == 'ru':\n",
    "                wiki_summary = get_wikipedia_summary(title)\n",
    "                if wiki_summary:\n",
    "                    extra_description = wiki_summary\n",
    "    elif 'wikidata' in tags:\n",
    "        wikidata_id = tags['wikidata']\n",
    "        wiki_description = get_wikidata_description(wikidata_id)\n",
    "        if wiki_description:\n",
    "            extra_description = wiki_description\n",
    "\n",
    "    if extra_description:\n",
    "        description_parts.append(extra_description)\n",
    "\n",
    "    return \". \".join(description_parts)\n",
    "\n",
    "def process_city_pois(city_name):\n",
    "    print(f\"\\n--- –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –≥–æ—Ä–æ–¥–∞: {city_name} ---\")\n",
    "    overpass_query = f\"\"\"\n",
    "    [out:json][timeout:180];\n",
    "    area[name=\"{city_name}\"]->.searchArea;\n",
    "    (\n",
    "      node[\"tourism\"](area.searchArea);\n",
    "      way[\"tourism\"](area.searchArea);\n",
    "\n",
    "      node[\"leisure\"](area.searchArea);\n",
    "      way[\"leisure\"](area.searchArea);\n",
    "\n",
    "      node[\"historic\"](area.searchArea);\n",
    "      way[\"historic\"](area.searchArea);\n",
    "\n",
    "      node[\"place\"~\"square|fountain\"](area.searchArea);\n",
    "      way[\"place\"~\"square|fountain\"](area.searchArea);\n",
    "\n",
    "      node[\"natural\"~\"park|wood|garden|beach|peak|water\"](area.searchArea);\n",
    "      way[\"natural\"~\"park|wood|garden|beach|peak|water\"](area.searchArea);\n",
    "\n",
    "      node[\"amenity\"~\"arts_centre|theatre|cinema|museum|library|nightclub|bar|restaurant|cafe|pub|food_court|community_centre|marketplace|atm|bank|clinic|hospital|pharmacy|post_office|police|fire_station|school|university|kindergarten|dentist|veterinary|parking|toilets|fountain|place_of_worship|courthouse|embassy|townhall|public_bath|sauna|stripclub|brothel\"](area.searchArea);\n",
    "      way[\"amenity\"~\"arts_centre|theatre|cinema|museum|library|nightclub|bar|restaurant|cafe|pub|food_court|community_centre|marketplace|place_of_worship\"](area.searchArea);\n",
    "      node[\"amenity\"~\"^(arts_centre|theatre|cinema|museum|library|nightclub|bar|restaurant|cafe|pub|food_court|marketplace|fountain|public_bath|sauna|stripclub|brothel)$\"](area.searchArea);\n",
    "      way[\"amenity\"~\"^(arts_centre|theatre|cinema|museum|library|nightclub|bar|restaurant|cafe|pub|food_court|marketplace|fountain|public_bath|sauna|stripclub|brothel)$\"](area.searchArea);\n",
    "\n",
    "      node[\"shop\"~\"^(mall|department_store|books|gift|souvenir|art|antiques|craft|boutique|jewelry|leather|music|shoes|toys|video)$\"](area.searchArea);\n",
    "      way[\"shop\"~\"^(mall|department_store|books|gift|souvenir|art|antiques|craft|boutique|jewelry|leather|music|shoes|toys|video)$\"](area.searchArea);\n",
    "    );\n",
    "    out body;\n",
    "    >;\n",
    "    out skel qt;\n",
    "    \"\"\"\n",
    "\n",
    "    amenities_to_include = [\n",
    "        \"arts_centre\", \"theatre\", \"cinema\", \"museum\", \"library\", \"nightclub\", \"bar\",\n",
    "        \"restaurant\", \"cafe\", \"pub\", \"food_court\", \"marketplace\", \"fountain\",\n",
    "        \"public_bath\", \"sauna\", \"stripclub\", \"brothel\", \"casino\", \"ferry_terminal\",\n",
    "        \"attraction\", \"theme_park\", \"water_park\", \"zoo\", \"aquarium\", \"planetarium\",\n",
    "        \"gallery\", \"viewpoint\", \"observatory\"\n",
    "    ]\n",
    "    shops_to_include = [\n",
    "        \"mall\", \"department_store\", \"books\", \"gift\", \"souvenir\", \"art\", \"antiques\",\n",
    "        \"craft\", \"boutique\", \"jewelry\", \"leather\", \"music\", \"shoes\", \"toys\", \"video\",\n",
    "        \"kiosk\", \"convenience\"\n",
    "    ]\n",
    "\n",
    "    amenity_regex = \"|\".join(amenities_to_include)\n",
    "    shop_regex = \"|\".join(shops_to_include)\n",
    "\n",
    "    overpass_query_filtered = f\"\"\"\n",
    "    [out:json][timeout:180];\n",
    "    area[name=\"{city_name}\"]->.searchArea;\n",
    "    (\n",
    "      node[\"tourism\"](area.searchArea);\n",
    "      way[\"tourism\"](area.searchArea);\n",
    "      node[\"leisure\"](area.searchArea);\n",
    "      way[\"leisure\"](area.searchArea);\n",
    "      node[\"historic\"](area.searchArea);\n",
    "      way[\"historic\"](area.searchArea);\n",
    "\n",
    "      node[\"place\"~\"square|fountain\"](area.searchArea);\n",
    "      way[\"place\"~\"square|fountain\"](area.searchArea);\n",
    "      node[\"natural\"~\"park|wood|garden|beach|peak|water\"](area.searchArea);\n",
    "      way[\"natural\"~\"park|wood|garden|beach|peak|water\"](area.searchArea);\n",
    "\n",
    "      node[\"amenity\"~\"^{amenity_regex}$\"](area.searchArea);\n",
    "      way[\"amenity\"~\"^{amenity_regex}$\"](area.searchArea);\n",
    "\n",
    "      node[\"shop\"~\"^{shop_regex}$\"](area.searchArea);\n",
    "      way[\"shop\"~\"^{shop_regex}$\"](area.searchArea);\n",
    "    );\n",
    "    out body;\n",
    "    >;\n",
    "    out skel qt;\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"üöÄ –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ Overpass API –¥–ª—è {city_name}...\")\n",
    "    try:\n",
    "        response = requests.get(OVERPASS_URL, params={'data': overpass_query_filtered}, timeout=180)\n",
    "        print(f\"‚úÖ –ó–∞–ø—Ä–æ—Å –¥–ª—è {city_name} –≤—ã–ø–æ–ª–Ω–µ–Ω —Å –∫–æ–¥–æ–º: {response.status_code}\")\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "        else:\n",
    "            print(f\"–û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è {city_name}: {response.text}\")\n",
    "            data = {'elements': []}\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(f\"–¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ Overpass API –¥–ª—è {city_name}\")\n",
    "        data = {'elements': []}\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ Overpass API –¥–ª—è {city_name}: {e}\")\n",
    "        data = {'elements': []}\n",
    "\n",
    "\n",
    "    current_city_pois = []\n",
    "    print(f\"‚öôÔ∏è –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É {len(data['elements'])} –æ–±—ä–µ–∫—Ç–æ–≤ –¥–ª—è {city_name}...\")\n",
    "\n",
    "    for element in tqdm(data['elements'], desc=f\"–û–±—Ä–∞–±–æ—Ç–∫–∞ POI –≤ {city_name}\"):\n",
    "        if 'tags' in element:\n",
    "            tags = element['tags']\n",
    "\n",
    "            if 'name' not in tags:\n",
    "                continue\n",
    "\n",
    "            lat, lon = (0, 0)\n",
    "            if element['type'] == 'node':\n",
    "                lat = element.get('lat')\n",
    "                lon = element.get('lon')\n",
    "            elif 'center' in element:\n",
    "                lat = element['center'].get('lat')\n",
    "                lon = element['center'].get('lon')\n",
    "\n",
    "            if lat == 0 and lon == 0:\n",
    "                continue\n",
    "\n",
    "            text_description = create_text_description(tags)\n",
    "\n",
    "            current_city_pois.append({\n",
    "                'id': element['id'],\n",
    "                'type': element['type'],\n",
    "                'lat': lat,\n",
    "                'lon': lon,\n",
    "                'name': tags.get('name'),\n",
    "                'city': city_name,\n",
    "                'text_description': text_description,\n",
    "                'tags': tags\n",
    "            })\n",
    "\n",
    "        time.sleep(0.005) # 5 –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥\n",
    "\n",
    "    print(f\"üëç –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(current_city_pois)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤ —Å –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏ –¥–ª—è {city_name}.\")\n",
    "\n",
    "    all_processed_pois.extend(current_city_pois)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cities_to_process = [\"–ö–∞–∑–∞–Ω—å\", \"–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥\", \"–ú–æ—Å–∫–≤–∞\"]\n",
    "\n",
    "    for city in cities_to_process:\n",
    "        process_city_pois(city)\n",
    "        time.sleep(10)\n",
    "\n",
    "    print(\"\\n--- –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö –≥–æ—Ä–æ–¥–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–∞! ---\")\n",
    "    print(f\"–í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ {len(all_processed_pois)} POI.\")\n",
    "\n",
    "    if all_processed_pois:\n",
    "        final_df = pd.DataFrame(all_processed_pois)\n",
    "        final_df.drop_duplicates(subset=['id'], inplace=True)\n",
    "        print(f\"–í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö POI –ø–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {len(final_df)}\")\n",
    "\n",
    "        output_filename = 'poi_dataset_russia_filtered_enriched.csv'\n",
    "        final_df.to_csv(output_filename, index=False)\n",
    "        print(f\"üíæ –û–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ '{output_filename}'\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è –û–±—â–∏–π –¥–∞—Ç–∞—Å–µ—Ç –ø—É—Å—Ç. –í–æ–∑–º–æ–∂–Ω–æ, –≤–æ–∑–Ω–∏–∫–ª–∏ –æ—à–∏–±–∫–∏ –ø—Ä–∏ —Å–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö.\")"
   ],
   "id": "ee5714ef775ca054",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –≥–æ—Ä–æ–¥–∞: –ö–∞–∑–∞–Ω—å ---\n",
      "üöÄ –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ Overpass API –¥–ª—è –ö–∞–∑–∞–Ω—å...\n",
      "‚úÖ –ó–∞–ø—Ä–æ—Å –¥–ª—è –ö–∞–∑–∞–Ω—å –≤—ã–ø–æ–ª–Ω–µ–Ω —Å –∫–æ–¥–æ–º: 200\n",
      "‚öôÔ∏è –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É 73622 –æ–±—ä–µ–∫—Ç–æ–≤ –¥–ª—è –ö–∞–∑–∞–Ω—å...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "–û–±—Ä–∞–±–æ—Ç–∫–∞ POI –≤ –ö–∞–∑–∞–Ω—å: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 73622/73622 [06:29<00:00, 189.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üëç –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 3060 —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤ —Å –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏ –¥–ª—è –ö–∞–∑–∞–Ω—å.\n",
      "\n",
      "--- –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –≥–æ—Ä–æ–¥–∞: –°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥ ---\n",
      "üöÄ –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ Overpass API –¥–ª—è –°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥...\n",
      "‚úÖ –ó–∞–ø—Ä–æ—Å –¥–ª—è –°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥ –≤—ã–ø–æ–ª–Ω–µ–Ω —Å –∫–æ–¥–æ–º: 200\n",
      "‚öôÔ∏è –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É 413806 –æ–±—ä–µ–∫—Ç–æ–≤ –¥–ª—è –°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "–û–±—Ä–∞–±–æ—Ç–∫–∞ POI –≤ –°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 413806/413806 [37:42<00:00, 182.87it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üëç –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 16858 —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤ —Å –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏ –¥–ª—è –°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥.\n",
      "\n",
      "--- –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –≥–æ—Ä–æ–¥–∞: –ú–æ—Å–∫–≤–∞ ---\n",
      "üöÄ –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ Overpass API –¥–ª—è –ú–æ—Å–∫–≤–∞...\n",
      "‚úÖ –ó–∞–ø—Ä–æ—Å –¥–ª—è –ú–æ—Å–∫–≤–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω —Å –∫–æ–¥–æ–º: 200\n",
      "‚öôÔ∏è –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É 608685 –æ–±—ä–µ–∫—Ç–æ–≤ –¥–ª—è –ú–æ—Å–∫–≤–∞...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "–û–±—Ä–∞–±–æ—Ç–∫–∞ POI –≤ –ú–æ—Å–∫–≤–∞: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 608685/608685 [54:07<00:00, 187.45it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üëç –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 21178 —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤ —Å –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏ –¥–ª—è –ú–æ—Å–∫–≤–∞.\n",
      "\n",
      "--- –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö –≥–æ—Ä–æ–¥–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–∞! ---\n",
      "–í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ 41096 POI.\n",
      "–í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö POI –ø–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: 41087\n",
      "üíæ –û–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ 'poi_dataset_russia_filtered_enriched.csv'\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T12:15:10.219522Z",
     "start_time": "2025-06-17T11:58:56.662281Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "OVERPASS_URL = \"http://overpass-api.de/api/interpreter\"\n",
    "WIKIPEDIA_API_URL = \"https://ru.wikipedia.org/w/api.php\"\n",
    "WIKIDATA_API_URL = \"https://www.wikidata.org/w/api.php\"\n",
    "\n",
    "def get_wikipedia_summary(title):\n",
    "    try:\n",
    "        params = {\n",
    "            \"action\": \"query\", \"format\": \"json\", \"titles\": title,\n",
    "            \"prop\": \"extracts\", \"exintro\": True, \"explaintext\": True, \"redirects\": 1\n",
    "        }\n",
    "        response = requests.get(WIKIPEDIA_API_URL, params=params, timeout=10)\n",
    "        data = response.json()\n",
    "        page_id = next(iter(data['query']['pages']))\n",
    "        if page_id != '-1':\n",
    "            return data['query']['pages'][page_id].get('extract', '')\n",
    "        return \"\"\n",
    "    except requests.exceptions.Timeout:\n",
    "        return \"\"\n",
    "    except requests.exceptions.RequestException:\n",
    "        return \"\"\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def get_wikidata_description(qid):\n",
    "    try:\n",
    "        params = {\n",
    "            \"action\": \"wbgetentities\", \"format\": \"json\", \"ids\": qid,\n",
    "            \"props\": \"descriptions\", \"languages\": \"ru|en\"\n",
    "        }\n",
    "        response = requests.get(WIKIDATA_API_URL, params=params, timeout=10)\n",
    "        data = response.json()\n",
    "        entity = data['entities'].get(qid)\n",
    "        if entity and 'descriptions' in entity:\n",
    "            if 'ru' in entity['descriptions']:\n",
    "                return entity['descriptions']['ru']['value']\n",
    "            elif 'en' in entity['descriptions']:\n",
    "                return entity['descriptions']['en']['value']\n",
    "        return \"\"\n",
    "    except requests.exceptions.Timeout:\n",
    "        return \"\"\n",
    "    except requests.exceptions.RequestException:\n",
    "        return \"\"\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def create_text_description(tags):\n",
    "    description_parts = []\n",
    "    if 'name' in tags:\n",
    "        description_parts.append(tags['name'])\n",
    "\n",
    "    tag_translations = {\n",
    "        'amenity': '–¢–∏–ø', 'shop': '–ú–∞–≥–∞–∑–∏–Ω', 'tourism': '–¢—É—Ä–∏–∑–º',\n",
    "        'leisure': '–î–æ—Å—É–≥', 'historic': '–ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–π –æ–±—ä–µ–∫—Ç', 'cuisine': '–ö—É—Ö–Ω—è'\n",
    "    }\n",
    "    for key, name in tag_translations.items():\n",
    "        if key in tags:\n",
    "            description_parts.append(f\"{name}: {tags[key].replace('_', ' ')}\")\n",
    "\n",
    "    if 'opening_hours' in tags: description_parts.append(f\"–ß–∞—Å—ã —Ä–∞–±–æ—Ç—ã: {tags['opening_hours']}\")\n",
    "    if 'wheelchair' in tags and tags['wheelchair'] == 'yes': description_parts.append(\"–î–æ—Å—Ç—É–ø–Ω–æ –¥–ª—è –∏–Ω–≤–∞–ª–∏–¥–Ω—ã—Ö –∫–æ–ª—è—Å–æ–∫\")\n",
    "    if 'internet_access' in tags and tags['internet_access'] == 'yes': description_parts.append(\"–ï—Å—Ç—å –¥–æ—Å—Ç—É–ø –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç\")\n",
    "    if 'phone' in tags: description_parts.append(f\"–¢–µ–ª–µ—Ñ–æ–Ω: {tags['phone']}\")\n",
    "    if 'website' in tags: description_parts.append(f\"–í–µ–±—Å–∞–π—Ç: {tags['website']}\")\n",
    "    if 'description' in tags: description_parts.append(f\"–û–ø–∏—Å–∞–Ω–∏–µ: {tags['description']}\")\n",
    "\n",
    "    extra_description = \"\"\n",
    "    if 'wikipedia' in tags:\n",
    "        parts = tags['wikipedia'].split(':')\n",
    "        if len(parts) >= 2:\n",
    "            lang = parts[0]\n",
    "            title = parts[-1]\n",
    "            if lang == 'ru':\n",
    "                wiki_summary = get_wikipedia_summary(title)\n",
    "                if wiki_summary: extra_description = wiki_summary\n",
    "    elif 'wikidata' in tags:\n",
    "        wikidata_id = tags['wikidata']\n",
    "        wiki_description = get_wikidata_description(wikidata_id)\n",
    "        if wiki_description: extra_description = wiki_description\n",
    "\n",
    "    if extra_description: description_parts.append(extra_description)\n",
    "    return \". \".join(description_parts)\n",
    "\n",
    "def process_city_pois(city_name):\n",
    "    print(f\"\\n--- –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –≥–æ—Ä–æ–¥–∞: {city_name} ---\")\n",
    "\n",
    "    amenities_to_include = [\n",
    "        \"arts_centre\", \"theatre\", \"cinema\", \"museum\", \"library\", \"nightclub\", \"bar\",\n",
    "        \"restaurant\", \"cafe\", \"pub\", \"food_court\", \"marketplace\", \"fountain\",\n",
    "        \"public_bath\", \"sauna\", \"casino\", \"ferry_terminal\", \"attraction\",\n",
    "        \"theme_park\", \"water_park\", \"zoo\", \"aquarium\", \"planetarium\", \"gallery\",\n",
    "        \"viewpoint\", \"observatory\", \"place_of_worship\", \"community_centre\", \"social_facility\"\n",
    "    ]\n",
    "    shops_to_include = [\n",
    "        \"mall\", \"department_store\", \"books\", \"gift\", \"souvenir\", \"art\", \"antiques\",\n",
    "        \"craft\", \"boutique\", \"jewelry\", \"leather\", \"music\", \"shoes\", \"toys\", \"video\",\n",
    "        \"convenience\", \"supermarket\", \"bakery\", \"beverages\", \"confectionery\", \"deli\",\n",
    "        \"farm\", \"greengrocer\", \"ice_cream\", \"pastry\", \"wine\", \"stationery\", \"sports\",\n",
    "        \"fashion\", \"perfumery\"\n",
    "    ]\n",
    "\n",
    "    amenity_filter = \"|\".join(amenities_to_include)\n",
    "    shop_filter = \"|\".join(shops_to_include)\n",
    "\n",
    "    overpass_query_strict = f\"\"\"\n",
    "    [out:json][timeout:180];\n",
    "    area[name=\"{city_name}\"]->.searchArea;\n",
    "    (\n",
    "      node[\"tourism\"](area.searchArea);\n",
    "      way[\"tourism\"](area.searchArea);\n",
    "      relation[\"tourism\"](area.searchArea);\n",
    "\n",
    "      node[\"leisure\"](area.searchArea);\n",
    "      way[\"leisure\"](area.searchArea);\n",
    "      relation[\"leisure\"](area.searchArea);\n",
    "\n",
    "      node[\"historic\"](area.searchArea);\n",
    "      way[\"historic\"](area.searchArea);\n",
    "      relation[\"historic\"](area.searchArea);\n",
    "\n",
    "      node[\"place\"~\"square|fountain\"](area.searchArea);\n",
    "      way[\"place\"~\"square|fountain\"](area.searchArea);\n",
    "\n",
    "      node[\"natural\"~\"park|wood|garden|beach|peak|water|forest|island|ridge|valley|volcano|wetland|glacier\"](area.searchArea);\n",
    "      way[\"natural\"~\"park|wood|garden|beach|peak|water|forest|island|ridge|valley|volcano|wetland|glacier\"](area.searchArea);\n",
    "      relation[\"natural\"~\"park|wood|garden|beach|peak|water|forest|island|ridge|valley|volcano|wetland|glacier\"](area.searchArea);\n",
    "\n",
    "      node[\"amenity\"~\"^{amenity_filter}$\"](area.searchArea);\n",
    "      way[\"amenity\"~\"^{amenity_filter}$\"](area.searchArea);\n",
    "\n",
    "      node[\"shop\"~\"^{shop_filter}$\"](area.searchArea);\n",
    "      way[\"shop\"~\"^{shop_filter}$\"](area.searchArea);\n",
    "\n",
    "      node[\"landuse\"~\"forest|park|recreation_ground|village_green\"](area.searchArea);\n",
    "      way[\"landuse\"~\"forest|park|recreation_ground|village_green\"](area.searchArea);\n",
    "    );\n",
    "    out body;\n",
    "    >;\n",
    "    out skel qt;\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"üöÄ –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å –∫ Overpass API –¥–ª—è {city_name}...\")\n",
    "    try:\n",
    "        response = requests.get(OVERPASS_URL, params={'data': overpass_query_strict}, timeout=180)\n",
    "        print(f\"‚úÖ –ó–∞–ø—Ä–æ—Å –¥–ª—è {city_name} –≤—ã–ø–æ–ª–Ω–µ–Ω —Å –∫–æ–¥–æ–º: {response.status_code}\")\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "        else:\n",
    "            print(f\"–û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è {city_name}: {response.text}\")\n",
    "            data = {'elements': []}\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(f\"–¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ Overpass API –¥–ª—è {city_name}. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —É–≤–µ–ª–∏—á–∏—Ç—å timeout.\")\n",
    "        data = {'elements': []}\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ Overpass API –¥–ª—è {city_name}: {e}\")\n",
    "        data = {'elements': []}\n",
    "\n",
    "    current_city_pois = []\n",
    "    print(f\"‚öôÔ∏è –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É {len(data['elements'])} –æ–±—ä–µ–∫—Ç–æ–≤, –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö –∏–∑ Overpass –¥–ª—è {city_name}...\")\n",
    "\n",
    "    for element in tqdm(data['elements'], desc=f\"–û–±—Ä–∞–±–æ—Ç–∫–∞ –∏ –æ–±–æ–≥–∞—â–µ–Ω–∏–µ POI –≤ {city_name}\"):\n",
    "        if 'tags' in element:\n",
    "            tags = element['tags']\n",
    "\n",
    "            if 'name' not in tags:\n",
    "                continue\n",
    "\n",
    "            lat, lon = (0, 0)\n",
    "            if element['type'] == 'node':\n",
    "                lat = element.get('lat')\n",
    "                lon = element.get('lon')\n",
    "            elif element['type'] == 'way' and 'center' in element:\n",
    "                lat = element['center'].get('lat')\n",
    "                lon = element['center'].get('lon')\n",
    "            elif element['type'] == 'relation' and 'center' in element:\n",
    "                lat = element['center'].get('lat')\n",
    "                lon = element['center'].get('lon')\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            if lat == 0 and lon == 0:\n",
    "                continue\n",
    "\n",
    "            text_description = create_text_description(tags)\n",
    "\n",
    "            current_city_pois.append({\n",
    "                'id': element['id'],\n",
    "                'type': element['type'],\n",
    "                'lat': lat,\n",
    "                'lon': lon,\n",
    "                'name': tags.get('name'),\n",
    "                'city': city_name,\n",
    "                'text_description': text_description,\n",
    "                'tags': tags\n",
    "            })\n",
    "\n",
    "        time.sleep(0.005)\n",
    "\n",
    "    print(f\"üëç –í—Å–µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∏ –¥–æ–±–∞–≤–ª–µ–Ω–æ –≤ —Å–ø–∏—Å–æ–∫: {len(current_city_pois)} POI –¥–ª—è {city_name}.\")\n",
    "\n",
    "    return current_city_pois\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    output_filename = 'poi_dataset_russia_filtered_enriched.csv'\n",
    "\n",
    "    # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –¥–∞—Ç–∞—Å–µ—Ç, –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å\n",
    "    if os.path.exists(output_filename):\n",
    "        print(f\"–ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ '{output_filename}'...\")\n",
    "        existing_df = pd.read_csv(output_filename)\n",
    "        print(f\"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(existing_df)} POI –∏–∑ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ —Ñ–∞–π–ª–∞.\")\n",
    "    else:\n",
    "        print(f\"–§–∞–π–ª '{output_filename}' –Ω–µ –Ω–∞–π–¥–µ–Ω. –°–æ–∑–¥–∞–¥–∏–º –Ω–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç.\")\n",
    "        existing_df = pd.DataFrame()\n",
    "\n",
    "    cities_to_add = [\"–ï–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥\"]\n",
    "\n",
    "    newly_collected_pois = []\n",
    "    for city in cities_to_add:\n",
    "        city_data = process_city_pois(city)\n",
    "        newly_collected_pois.extend(city_data)\n",
    "        time.sleep(10)\n",
    "\n",
    "    print(\"\\n--- –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –Ω–æ–≤—ã—Ö –≥–æ—Ä–æ–¥–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω! ---\")\n",
    "    print(f\"–í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ {len(newly_collected_pois)} –Ω–æ–≤—ã—Ö POI.\")\n",
    "\n",
    "    combined_pois = pd.concat([existing_df, pd.DataFrame(newly_collected_pois)], ignore_index=True)\n",
    "\n",
    "    print(f\"–í—Å–µ–≥–æ POI –¥–æ —É–¥–∞–ª–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {len(combined_pois)}\")\n",
    "    combined_pois.dropna(subset=['name', 'lat', 'lon'], inplace=True)\n",
    "    combined_pois.drop_duplicates(subset=['id'], inplace=True)\n",
    "\n",
    "    print(f\"–ò—Ç–æ–≥–æ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∏ –ø–æ–ª–Ω—ã—Ö POI: {len(combined_pois)}\")\n",
    "\n",
    "    combined_pois.to_csv(output_filename, index=False)\n",
    "    print(f\"üíæ –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ '{output_filename}'\")"
   ],
   "id": "b39d2a53a961737b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ 'poi_dataset_russia_filtered_enriched.csv'...\n",
      "–ó–∞–≥—Ä—É–∂–µ–Ω–æ 46354 POI –∏–∑ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ —Ñ–∞–π–ª–∞.\n",
      "\n",
      "--- –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –≥–æ—Ä–æ–¥–∞: –ï–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥ ---\n",
      "üöÄ –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å –∫ Overpass API –¥–ª—è –ï–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥...\n",
      "‚úÖ –ó–∞–ø—Ä–æ—Å –¥–ª—è –ï–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥ –≤—ã–ø–æ–ª–Ω–µ–Ω —Å –∫–æ–¥–æ–º: 200\n",
      "‚öôÔ∏è –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É 173809 –æ–±—ä–µ–∫—Ç–æ–≤, –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö –∏–∑ Overpass –¥–ª—è –ï–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "–û–±—Ä–∞–±–æ—Ç–∫–∞ –∏ –æ–±–æ–≥–∞—â–µ–Ω–∏–µ POI –≤ –ï–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 173809/173809 [15:47<00:00, 183.35it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üëç –í—Å–µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∏ –¥–æ–±–∞–≤–ª–µ–Ω–æ –≤ —Å–ø–∏—Å–æ–∫: 4109 POI –¥–ª—è –ï–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥.\n",
      "\n",
      "--- –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –Ω–æ–≤—ã—Ö –≥–æ—Ä–æ–¥–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω! ---\n",
      "–í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ 4109 –Ω–æ–≤—ã—Ö POI.\n",
      "–í—Å–µ–≥–æ POI –¥–æ —É–¥–∞–ª–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: 50463\n",
      "–ò—Ç–æ–≥–æ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∏ –ø–æ–ª–Ω—ã—Ö POI: 50463\n",
      "üíæ –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ 'poi_dataset_russia_filtered_enriched.csv'\n"
     ]
    }
   ],
   "execution_count": 13
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
